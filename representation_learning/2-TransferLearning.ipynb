{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller _Representation Learning_: Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTA: Para que funcione el codigo hay que descargar el dataset.\n",
    "\n",
    "Para descargar el Flickr8K dataset:\n",
    "[https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip](https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip).\n",
    "Si ese link ya no funciona hay que seguir elp proceso y llenar el formulario [aqui](https://forms.illinois.edu/sec/1713398).\n",
    "\n",
    "- Extraer el ZIP en el directorio `data`\n",
    "- Ademas hay que descargar los _captions_ del dataset [aqui](http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip). Extrar en `caption_datasets`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues de aprender la tarea de \"generar _captions_\", ahora utilizaremos ese \"cortex\" para resolver tareas relaciondaas para las cuales el modelo no fue entrenado pero tambien \"aprendio\". Algunas de estas son:\n",
    "\n",
    "- Hacer algebra en la semantica conceptual de las palabras\n",
    "\n",
    "- Encontrar imagenes semanticamente similares\n",
    "\n",
    "- Encontrar imagenes a partir de una descripcion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "import IPython.display\n",
    "from math import floor\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.nn.functional as F           # layers, activations and more\n",
    "import torch.optim as optim  \n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(is_cuda):\n",
    "    USE_GPU = True\n",
    "else:\n",
    "    USE_GPU = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes import INCEPTION as inception\n",
    "from classes import \\\n",
    "    ENDWORD, STARTWORD, PADWORD, HEIGHT, WIDTH, \\\n",
    "    INPUT_EMBEDDING, HIDDEN_SIZE, OUTPUT_EMBEDDING, \\\n",
    "    CAPTION_FILE, IMAGE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando InceptionV3 pre-entrenada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception.load_state_dict(torch.load('models/inception_epochs_40.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(USE_GPU):\n",
    "    inception.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase para iterar en los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f = pickle.load(open(\"pickles/flickr_data_loader.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes import IC_V6\n",
    "\n",
    "net = IC_V6(f.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load('models/epochs_40_loss_2_841_v6.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(USE_GPU):\n",
    "    net.cuda()\n",
    "    inception.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IC_V6(\n",
       "  (batchnorm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (input_embedding): Embedding(8385, 300)\n",
       "  (embedding_dropout): Dropout(p=0.22, inplace=False)\n",
       "  (gru): GRU(300, 300, num_layers=3, dropout=0.22)\n",
       "  (linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (out): Linear(in_features=300, out_features=8385, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizando los embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_threshold = 50 # the word should have appeared at least this many times for us to visualize\n",
    "\n",
    "all_word_embeddings = []\n",
    "all_words = []\n",
    "\n",
    "for word in f.word_frequency.keys():\n",
    "    if(f.word_frequency[word] >= frequency_threshold):\n",
    "        all_word_embeddings.append(net.input_embedding(torch.tensor(f.w2i[word])).detach().numpy())\n",
    "        all_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "701"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando T-SNE (http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) para visualizar el embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2d = tsne.fit_transform(all_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_cmap = rand_cmap(10, type='bright', first_color_black=True, last_color_black=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_annot(ind):\n",
    "\n",
    "    pos = sc.get_offsets()[ind[\"ind\"][0]]\n",
    "    annot.xy = pos\n",
    "    text = \"{}\".format(\" \".join([all_words[n] for n in ind[\"ind\"]]))\n",
    "    annot.set_text(text)\n",
    "    annot.get_bbox_patch().set_facecolor('white')\n",
    "    annot.get_bbox_patch().set_alpha(0.9)\n",
    "\n",
    "\n",
    "def hover(event):\n",
    "    \n",
    "    vis = annot.get_visible()\n",
    "    if event.inaxes == ax:\n",
    "        cont, ind = sc.contains(event)\n",
    "        if cont:\n",
    "            update_annot(ind)\n",
    "            annot.set_visible(True)\n",
    "            fig.canvas.draw_idle()\n",
    "        else:\n",
    "            if vis:\n",
    "                annot.set_visible(False)\n",
    "                fig.canvas.draw_idle()\n",
    "                \n",
    "def onpick(event):\n",
    "    ind = event.ind\n",
    "    print(ind)\n",
    "    label_pos_x = event.mouseevent.xdata\n",
    "    label_pos_y = event.mouseevent.ydata\n",
    "    annot.xy = (label_pos_x,label_pos_y)\n",
    "    annot.set_text(y[ind])\n",
    "    ax.figure.canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72303706dd8441a8b1c381e66299338d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(figsize=(12, 12))\n",
    "    \n",
    "sc = plt.scatter(X_2d[:,0], X_2d[:,1])\n",
    "#plt.legend()\n",
    "#plt.show()\n",
    "\n",
    "annot = ax.annotate(\"\", xy=(0,0), xytext=(20,20),textcoords=\"offset points\",\n",
    "                    bbox=dict(boxstyle=\"round\", fc=\"w\"),\n",
    "                    arrowprops=dict(arrowstyle=\"->\", color='red'))\n",
    "annot.set_visible(False)\n",
    "fig.canvas.mpl_connect(\"motion_notify_event\", hover)\n",
    "#fig.canvas.mpl_connect('pick_event', onpick)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algebra en los embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrar las palabras mas similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_utils import return_cosine_sorted, return_similar_words, \\\n",
    "    return_embedding, return_analogy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'person'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '_'\n",
    "while len(query) < 5:\n",
    "    query = np.random.choice(all_words)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['girl', '0.18318498134613037'],\n",
       "       ['cat', '0.17168444395065308'],\n",
       "       ['shoes', '0.17018908262252808'],\n",
       "       ['leaping', '0.16315896809101105'],\n",
       "       ['cowboy', '0.16065259277820587']], dtype='<U32')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_similar_words('person', all_words, all_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_analogy('earth', 'brown', 'sky', all_words, all_word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizar embeddings de imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import cart2pol, pol2cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "inception.eval()\n",
    "\n",
    "try:\n",
    "    all_image_embeddings = pickle.load(open('pickles/all_image_embeddings.pkl', 'rb'))\n",
    "    all_image_filenames = pickle.load(open('pickles/all_image_filenames.pkl', 'rb'))\n",
    "except Exception as e:\n",
    "    print(\"> error loading data:\", e)\n",
    "    all_image_embeddings = []\n",
    "    all_image_filenames = []\n",
    "    for i in range(len(f.training_data)):\n",
    "        all_image_embeddings.append(\n",
    "            inception(f.image_to_tensor('data/'+f.training_data[i]['filename'])).detach().numpy())\n",
    "        all_image_filenames.append(f.training_data[i]['filename'])\n",
    "    pickle.dump(all_image_embeddings, open('pickles/all_image_embeddings.pkl', 'wb'))\n",
    "    pickle.dump(all_image_filenames, open('pickles/all_image_filenames.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_embeddings_temp = all_image_embeddings[:]\n",
    "all_image_filenames_temp = all_image_filenames[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import (TextArea, DrawingArea, OffsetImage,\n",
    "                                  AnnotationBbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne_images = TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2d = tsne.fit_transform(np.squeeze(all_image_embeddings_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03e8945c08c437789e86a8c02b6df80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(figsize=(10, 10))\n",
    "sc = plt.scatter(X_2d[:,0], X_2d[:,1])\n",
    "annot = ax.annotate(\"\", xy=(0,0), xytext=(20,20),textcoords=\"offset points\",\n",
    "                    bbox=dict(boxstyle=\"round\", fc=\"w\"),\n",
    "                    arrowprops=dict(arrowstyle=\"->\", color='red'))\n",
    "annot.set_visible(False)\n",
    "\n",
    "def update_annot(ind):\n",
    "    pos = sc.get_offsets()[ind[\"ind\"][0]]\n",
    "    annot.xy = pos\n",
    "    #text = \"{}\".format(\" \".join([all_words[n] for n in ind[\"ind\"]]))\n",
    "    #annot.set_text(text)\n",
    "    \n",
    "    rho = 10 #how for to draw centers of new images\n",
    "    total_radians = 2* np.pi\n",
    "    num_images = len(ind[\"ind\"])\n",
    "    if(num_images > 4): #at max 4\n",
    "        num_images=4\n",
    "    radians_offset = total_radians/num_images\n",
    "    for i in range(num_images):\n",
    "        hovered_filename = 'data/'+all_image_filenames_temp[ind[\"ind\"][i]]\n",
    "        arr_img = Image.open(hovered_filename, 'r')\n",
    "        imagebox = OffsetImage(arr_img, zoom=0.3)\n",
    "        #imagebox.image.axes = ax\n",
    "        offset = pol2cart(rho, i*radians_offset)\n",
    "        new_xy = (pos[0]+offset[0], pos[1]+offset[1])\n",
    "        ab = AnnotationBbox(imagebox, new_xy)\n",
    "        ax.add_artist(ab)  \n",
    "        annot.get_bbox_patch().set_facecolor('white')\n",
    "        annot.get_bbox_patch().set_alpha(0.9)\n",
    "\n",
    "\n",
    "def hover(event):\n",
    "    vis = annot.get_visible()\n",
    "    if event.inaxes == ax:\n",
    "        cont, ind = sc.contains(event)\n",
    "        if cont:\n",
    "            update_annot(ind)\n",
    "            annot.set_visible(True)\n",
    "            fig.canvas.draw_idle()\n",
    "        else:\n",
    "            if vis:\n",
    "                annot.set_visible(False)\n",
    "                remove_all_images()\n",
    "                fig.canvas.draw_idle()\n",
    "\n",
    "def remove_all_images():\n",
    "    for obj in ax.findobj(match = type(AnnotationBbox(1, 1))):\n",
    "        obj.remove()\n",
    "\n",
    "fig.canvas.mpl_connect(\"motion_notify_event\", hover)\n",
    "#fig.canvas.mpl_connect('pick_event', onpick)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrar imagenes similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(filename):\n",
    "    pil_im = Image.open(filename, 'r')\n",
    "    plt.figure()\n",
    "    plt.imshow(np.asarray(pil_im))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "\n",
    "def return_embedding_image(image_filename):\n",
    "    return inception(f.image_to_tensor(image_filename)).detach().numpy().squeeze()\n",
    "\n",
    "def return_similar_images(image_filename, top_n=5):\n",
    "    return return_cosine_sorted_image(return_embedding_image(image_filename))[1:top_n+1]\n",
    "    \n",
    "def return_cosine_sorted_image(target_image_embedding):\n",
    "    cosines = []\n",
    "    for i in range(len(all_image_embeddings)):\n",
    "        cosines.append(1 - spatial.distance.cosine(target_image_embedding, all_image_embeddings[i]))    \n",
    "    sorted_indexes = np.argsort(cosines)[::-1]\n",
    "    return np.vstack((np.array(all_image_filenames)[sorted_indexes], np.array(cosines)[sorted_indexes])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f62b5aa0bd4aada8897e59270e719b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_filename = 'custom_images/kite.jpg'\n",
    "plot_image(search_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images = return_similar_images(search_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77d32f711e247439b2787057de950a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image('data/'+similar_images[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscar imagenes con una frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentence = 'a kid playing'\n",
    "tokens= f.convert_sentence_to_tokens(target_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes import set_parameter_requires_grad, INPUT_EMBEDDING\n",
    "\n",
    "set_parameter_requires_grad(net, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_tensor = torch.autograd.Variable(torch.randn(1, INPUT_EMBEDDING)*0.01, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 300])\n"
     ]
    }
   ],
   "source": [
    "print(embedding_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Epoch:  0  loss:  17.016  | running avg loss:  17.016\n",
      "[['3108544687_c7115823f5.jpg' '0.14893005788326263']\n",
      " ['1806580620_a8fe0fb9f8.jpg' '0.1461697816848755']]\n",
      "==== Epoch:  10  loss:  14.165  | running avg loss:  15.294\n",
      "==== Epoch:  20  loss:  12.042  | running avg loss:  14.169\n",
      "==== Epoch:  30  loss:  10.97  | running avg loss:  13.261\n",
      "==== Epoch:  40  loss:  10.513  | running avg loss:  12.638\n",
      "==== Epoch:  50  loss:  10.026  | running avg loss:  12.171\n",
      "==== Epoch:  60  loss:  9.548  | running avg loss:  11.775\n",
      "==== Epoch:  70  loss:  9.169  | running avg loss:  11.431\n",
      "==== Epoch:  80  loss:  8.814  | running avg loss:  11.128\n",
      "==== Epoch:  90  loss:  8.497  | running avg loss:  10.853\n",
      "[['3527715826_ea5b4e8de4.jpg' '0.16713720560073853']\n",
      " ['3208032657_27b9d6c4f3.jpg' '0.16551019251346588']]\n"
     ]
    }
   ],
   "source": [
    "epochs = 100 # best at > 10**5\n",
    "loss_so_far = 0.0\n",
    "lr = 0.001\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    for epoch in range(epochs):\n",
    "        input_token = f.w2i[STARTWORD]\n",
    "        input_tensor = torch.tensor(input_token)\n",
    "        loss=0.\n",
    "        \n",
    "        # forward\n",
    "        for token in tokens:\n",
    "            if(input_token==f.w2i[STARTWORD]):\n",
    "                out, hidden=net(input_tensor, embedding_tensor, process_image=True, use_inception=False)\n",
    "            else:\n",
    "                out, hidden=net(input_tensor, hidden)\n",
    "            # current label\n",
    "            class_label = torch.tensor(token).view(1)\n",
    "            input_token = token\n",
    "            input_tensor = torch.tensor(input_token)\n",
    "            # predicted label\n",
    "            out = out.squeeze().view(1,-1)\n",
    "            loss += l(out, class_label)\n",
    "\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        #print(image_tensor.grad)\n",
    "        embedding_tensor = torch.autograd.Variable(embedding_tensor.clone() - lr * embedding_tensor.grad, requires_grad=True)\n",
    "        loss_so_far += loss.detach().item()\n",
    "\n",
    "        if(epoch %10 == 0):\n",
    "            print(\"==== Epoch: \",epoch, \" loss: \",round(loss.detach().item(),3),\" | running avg loss: \", round(loss_so_far/(epoch+1),3))\n",
    "            if(epoch %90 ==0):\n",
    "                similar_images = return_cosine_sorted_image(embedding_tensor.detach().numpy().squeeze())\n",
    "                print(similar_images[:2])\n",
    "                #plot_image('data/'+similar_images[0][0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f860db8ace74dd899c74bd400254b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image('data/3527715826_ea5b4e8de4.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
