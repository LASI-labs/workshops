{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller _Representation Learning_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTA: Para que funcione el codigo hay que descargar el dataset.\n",
    "\n",
    "Para descargar el Flickr8K dataset:\n",
    "[https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip](https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip).\n",
    "Si ese link ya no funciona hay que seguir elp proceso y llenar el formulario [aqui](https://forms.illinois.edu/sec/1713398).\n",
    "\n",
    "- Extraer el ZIP en el directorio `data`\n",
    "- Ademas hay que descargar los _captions_ del dataset [aqui](http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip). Extrar en `caption_datasets`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "import IPython.display\n",
    "from math import floor\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.nn.functional as F           # layers, activations and more\n",
    "import torch.optim as optim  \n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(is_cuda):\n",
    "    USE_GPU = True\n",
    "else:\n",
    "    USE_GPU = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes import INCEPTION as inception\n",
    "from classes import \\\n",
    "    ENDWORD, STARTWORD, PADWORD, HEIGHT, WIDTH, \\\n",
    "    INPUT_EMBEDDING, HIDDEN_SIZE, OUTPUT_EMBEDDING, \\\n",
    "    CAPTION_FILE, IMAGE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando InceptionV3 pre-entrenada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception.load_state_dict(torch.load('models/inception_epochs_40.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(USE_GPU):\n",
    "    inception.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase para iterar en los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f = pickle.load(open(\"pickles/flickr_data_loader.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes import IC_V6\n",
    "\n",
    "net = IC_V6(f.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('models/epochs_40_loss_2_841_v6.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(USE_GPU):\n",
    "    net.cuda()\n",
    "    inception.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizando los embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_threshold = 50 # the word should have appeared at least this many times for us to visualize\n",
    "\n",
    "all_word_embeddings = []\n",
    "all_words = []\n",
    "\n",
    "for word in f.word_frequency.keys():\n",
    "    if(f.word_frequency[word] >= frequency_threshold):\n",
    "        all_word_embeddings.append(net.input_embedding(torch.tensor(f.w2i[word])).detach().numpy())\n",
    "        all_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando T-SNE (http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) para visualizar el embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2d = tsne.fit_transform(all_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_cmap = rand_cmap(10, type='bright', first_color_black=True, last_color_black=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_annot(ind):\n",
    "\n",
    "    pos = sc.get_offsets()[ind[\"ind\"][0]]\n",
    "    annot.xy = pos\n",
    "    text = \"{}\".format(\" \".join([all_words[n] for n in ind[\"ind\"]]))\n",
    "    annot.set_text(text)\n",
    "    annot.get_bbox_patch().set_facecolor('white')\n",
    "    annot.get_bbox_patch().set_alpha(0.9)\n",
    "\n",
    "\n",
    "def hover(event):\n",
    "    \n",
    "    vis = annot.get_visible()\n",
    "    if event.inaxes == ax:\n",
    "        cont, ind = sc.contains(event)\n",
    "        if cont:\n",
    "            update_annot(ind)\n",
    "            annot.set_visible(True)\n",
    "            fig.canvas.draw_idle()\n",
    "        else:\n",
    "            if vis:\n",
    "                annot.set_visible(False)\n",
    "                fig.canvas.draw_idle()\n",
    "                \n",
    "def onpick(event):\n",
    "    ind = event.ind\n",
    "    print(ind)\n",
    "    label_pos_x = event.mouseevent.xdata\n",
    "    label_pos_y = event.mouseevent.ydata\n",
    "    annot.xy = (label_pos_x,label_pos_y)\n",
    "    annot.set_text(y[ind])\n",
    "    ax.figure.canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12, 12))\n",
    "    \n",
    "sc = plt.scatter(X_2d[:,0], X_2d[:,1])\n",
    "#plt.legend()\n",
    "#plt.show()\n",
    "\n",
    "annot = ax.annotate(\"\", xy=(0,0), xytext=(20,20),textcoords=\"offset points\",\n",
    "                    bbox=dict(boxstyle=\"round\", fc=\"w\"),\n",
    "                    arrowprops=dict(arrowstyle=\"->\", color='red'))\n",
    "annot.set_visible(False)\n",
    "fig.canvas.mpl_connect(\"motion_notify_event\", hover)\n",
    "#fig.canvas.mpl_connect('pick_event', onpick)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algebra en los embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrar las palabras mas similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_utils import return_cosine_sorted, return_similar_words, \\\n",
    "    return_embedding, return_analogy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '_'\n",
    "while len(query) < 5:\n",
    "    query = np.random.choice(all_words)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_similar_words('boy', all_words, all_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_analogy('grass', 'green', 'ground', all_words, all_word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizar embeddings de imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import cart2pol, pol2cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "inception.eval()\n",
    "\n",
    "try:\n",
    "    all_image_embeddings = pickle.load(open('pickles/all_image_embeddings.pkl', 'rb'))\n",
    "    all_image_filenames = pickle.load(open('pickles/all_image_filenames.pkl', 'rb'))\n",
    "except Exception as e:\n",
    "    print(\"> error loading data:\", e)\n",
    "    all_image_embeddings = []\n",
    "    all_image_filenames = []\n",
    "    for i in range(len(f.training_data)):\n",
    "        all_image_embeddings.append(\n",
    "            inception(f.image_to_tensor('data/'+f.training_data[i]['filename'])).detach().numpy())\n",
    "        all_image_filenames.append(f.training_data[i]['filename'])\n",
    "    pickle.dump(all_image_embeddings, open('pickles/all_image_embeddings.pkl', 'wb'))\n",
    "    pickle.dump(all_image_filenames, open('pickles/all_image_filenames.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_embeddings_temp = all_image_embeddings[:]\n",
    "all_image_filenames_temp = all_image_filenames[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import (TextArea, DrawingArea, OffsetImage,\n",
    "                                  AnnotationBbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne_images = TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2d = tsne.fit_transform(np.squeeze(all_image_embeddings_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10, 10))\n",
    "sc = plt.scatter(X_2d[:,0], X_2d[:,1])\n",
    "annot = ax.annotate(\"\", xy=(0,0), xytext=(20,20),textcoords=\"offset points\",\n",
    "                    bbox=dict(boxstyle=\"round\", fc=\"w\"),\n",
    "                    arrowprops=dict(arrowstyle=\"->\", color='red'))\n",
    "annot.set_visible(False)\n",
    "\n",
    "def update_annot(ind):\n",
    "    pos = sc.get_offsets()[ind[\"ind\"][0]]\n",
    "    annot.xy = pos\n",
    "    #text = \"{}\".format(\" \".join([all_words[n] for n in ind[\"ind\"]]))\n",
    "    #annot.set_text(text)\n",
    "    \n",
    "    rho = 10 #how for to draw centers of new images\n",
    "    total_radians = 2* np.pi\n",
    "    num_images = len(ind[\"ind\"])\n",
    "    if(num_images > 4): #at max 4\n",
    "        num_images=4\n",
    "    radians_offset = total_radians/num_images\n",
    "    for i in range(num_images):\n",
    "        hovered_filename = 'data/'+all_image_filenames_temp[ind[\"ind\"][i]]\n",
    "        arr_img = Image.open(hovered_filename, 'r')\n",
    "        imagebox = OffsetImage(arr_img, zoom=0.3)\n",
    "        #imagebox.image.axes = ax\n",
    "        offset = pol2cart(rho, i*radians_offset)\n",
    "        new_xy = (pos[0]+offset[0], pos[1]+offset[1])\n",
    "        ab = AnnotationBbox(imagebox, new_xy)\n",
    "        ax.add_artist(ab)  \n",
    "        annot.get_bbox_patch().set_facecolor('white')\n",
    "        annot.get_bbox_patch().set_alpha(0.9)\n",
    "\n",
    "\n",
    "def hover(event):\n",
    "    vis = annot.get_visible()\n",
    "    if event.inaxes == ax:\n",
    "        cont, ind = sc.contains(event)\n",
    "        if cont:\n",
    "            update_annot(ind)\n",
    "            annot.set_visible(True)\n",
    "            fig.canvas.draw_idle()\n",
    "        else:\n",
    "            if vis:\n",
    "                annot.set_visible(False)\n",
    "                remove_all_images()\n",
    "                fig.canvas.draw_idle()\n",
    "\n",
    "def remove_all_images():\n",
    "    for obj in ax.findobj(match = type(AnnotationBbox(1, 1))):\n",
    "        obj.remove()\n",
    "\n",
    "fig.canvas.mpl_connect(\"motion_notify_event\", hover)\n",
    "#fig.canvas.mpl_connect('pick_event', onpick)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar images to a given image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(filename):\n",
    "    pil_im = Image.open(filename, 'r')\n",
    "    plt.figure()\n",
    "    plt.imshow(np.asarray(pil_im))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "\n",
    "def return_embedding_image(image_filename):\n",
    "    return inception(f.image_to_tensor(image_filename)).detach().numpy().squeeze()\n",
    "\n",
    "def return_similar_images(image_filename, top_n=5):\n",
    "    return return_cosine_sorted_image(return_embedding_image(image_filename))[1:top_n+1]\n",
    "    \n",
    "def return_cosine_sorted_image(target_image_embedding):\n",
    "    cosines = []\n",
    "    for i in range(len(all_image_embeddings)):\n",
    "        cosines.append(1 - spatial.distance.cosine(target_image_embedding, all_image_embeddings[i]))    \n",
    "    sorted_indexes = np.argsort(cosines)[::-1]\n",
    "    return np.vstack((np.array(all_image_filenames)[sorted_indexes], np.array(cosines)[sorted_indexes])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_filename = 'custom_images/kite.jpg'\n",
    "plot_image(search_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_images = return_similar_images(search_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image('data/'+similar_images[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscar imagenes con una frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentence = 'a dog sleeping'\n",
    "tokens= f.convert_sentence_to_tokens(target_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes import set_parameter_requires_grad, INPUT_EMBEDDING\n",
    "\n",
    "set_parameter_requires_grad(net, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_tensor = torch.autograd.Variable(torch.randn(1, INPUT_EMBEDDING)*0.01, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 1000 # best at > 10**5\n",
    "loss_so_far = 0.0\n",
    "lr = 0.001\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    for epoch in range(epochs):\n",
    "        input_token = f.w2i[STARTWORD]\n",
    "        input_tensor = torch.tensor(input_token)\n",
    "        loss=0.\n",
    "        \n",
    "        # forward\n",
    "        for token in tokens:\n",
    "            if(input_token==f.w2i[STARTWORD]):\n",
    "                out, hidden=net(input_tensor, embedding_tensor, process_image=True, use_inception=False)\n",
    "            else:\n",
    "                out, hidden=net(input_tensor, hidden)\n",
    "            # current label\n",
    "            class_label = torch.tensor(token).view(1)\n",
    "            input_token = token\n",
    "            input_tensor = torch.tensor(input_token)\n",
    "            # predicted label\n",
    "            out = out.squeeze().view(1,-1)\n",
    "            loss += l(out, class_label)\n",
    "\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        #print(image_tensor.grad)\n",
    "        embedding_tensor = torch.autograd.Variable(embedding_tensor.clone() - lr * embedding_tensor.grad, requires_grad=True)\n",
    "        loss_so_far += loss.detach().item()\n",
    "\n",
    "        if(epoch %10 ==0):\n",
    "            print(\"==== Epoch: \",epoch, \" loss: \",round(loss.detach().item(),3),\" | running avg loss: \", round(loss_so_far/(epoch+1),3))\n",
    "            if(epoch %100 ==0):\n",
    "                similar_images = return_cosine_sorted_image(embedding_tensor.detach().numpy().squeeze())\n",
    "                print(similar_images[:2])\n",
    "                #plot_image('data/'+similar_images[0][0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image('data/832128857_1390386ea6.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
